{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbotgr8learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNchEcgQBXb4vKO4130bRi8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhura-Kamat/PracticeAssignment/blob/main/chatbotgr8learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoTa0LV8Tea3"
      },
      "source": [
        "Importing the required libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xldgH3jQFJH"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjSOvansTyBp"
      },
      "source": [
        "Importing And Reading the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn3wikXoQZaM",
        "outputId": "4d9f8092-a4f2-4c37-db80-d7fa91eaa8a3"
      },
      "source": [
        "f = open('chatbot.txt','r',errors='ignore')\n",
        "raw_doc = f.read()\n",
        "raw_doc = raw_doc.lower()# converts text to lowercase\n",
        "nltk.download('punkt') #using punkt tokenizer\n",
        "nltk.download('wordnet')#Using the wordNet Dictionary\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc)#converts doc to list of sentences\n",
        "word_tokens= nltk.word_tokenize(raw_doc)#converts doc to list of words\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_Nwvs34T8eG"
      },
      "source": [
        "Example Of sentence tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxfshn6hSWj5",
        "outputId": "cb8eaab7-c263-4120-fce0-b3845cf2b321"
      },
      "source": [
        "sent_tokens[:2]#print two  sentence tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.',\n",
              " 'data science is related to data mining, machine learning and big data.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjgfhVANUEh3"
      },
      "source": [
        "Example of word tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2s9nJvMSkqR",
        "outputId": "d07621a9-6df1-4f55-94f3-218610261995"
      },
      "source": [
        "word_tokens[:2]#print two word tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'science']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqAaa73fUMFY"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bqMHj7fUWZS"
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK\n",
        "def LemTokens(tokens):\n",
        "  return[lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQYmiVbfWpdq"
      },
      "source": [
        "Defining the greeting Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g8eierjWmhz"
      },
      "source": [
        "GREET_INPUTS = (\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\")\n",
        "GREET_RESPONSES =[\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glad!Tou are talking to me\"]\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOchgDP9YXKw"
      },
      "source": [
        "Response Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_I17TgKYeMd"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ta4QeBgvuLh"
      },
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize,stop_words='english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf==0): #if chatbot doesn't understand how to respond to input\n",
        "    robo1_response=robo1_response+\"I am sorry! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response =  robo1_response+sent_tokens[idx]\n",
        "    return  robo1_response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ6_wxjYAcA8"
      },
      "source": [
        "Defining conversation:start/end protocols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmxkaXQ3AhH3",
        "outputId": "eab7b46e-3fff-4721-b15c-82edd975663b"
      },
      "source": [
        "flag=True\n",
        "print(\"BOT:My name is stark.Let's have a conversation!if you want to exit any time,just type Bye!\")\n",
        "while(flag==True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response=='thanks' or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"BOT:You are welcome..\")\n",
        "    else:\n",
        "      if(greet(user_response)!=None):\n",
        "        print(\"BOT: \"+greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens = word_tokens+nltk.word_tokenize(user_response)\n",
        "        final_words=list(set(word_tokens))\n",
        "        print(\"BOT: \",end=\"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "\n",
        "  else:\n",
        "      flag=False\n",
        "      print(\"BOT: Goodbye! Take care \")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT:My name is stark.Let's have a conversation!if you want to exit any time,just type Bye!\n",
            "hi\n",
            "BOT: I am glad!Tou are talking to me\n",
            "foundation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: [6]\n",
            "\n",
            "\n",
            "contents\n",
            "1\tfoundations\n",
            "1.1\trelationship to statistics\n",
            "2\tetymology\n",
            "2.1\tearly usage\n",
            "2.2\tmodern usage\n",
            "3\tmarket\n",
            "4\ttechnologies and techniques\n",
            "4.1\ttechniques\n",
            "5\tsee also\n",
            "6\treferences\n",
            "foundations\n",
            "data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains.\n",
            "see also\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: I am sorry! I don't understand you\n",
            "modern usage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT: [23][21]\n",
            "\n",
            "modern usage\n",
            "the modern conception of data science as an independent discipline is sometimes attributed to william s.\n",
            "bye\n",
            "BOT: Goodbye! Take care \n"
          ]
        }
      ]
    }
  ]
}